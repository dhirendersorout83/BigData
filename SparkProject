https://databricks.com/blog/2017/04/04/real-time-end-to-end-integration-with-apache-kafka-in-apache-sparks-structured-streaming.html

// spark session to create stream
val streamInputDF = sparksession.readStream
.format("kafka")
.option("kafka.bootstrap.servers","<ip>:<port>,<ip>:<port>,<ip>:<port>")
.option("subscribe","<topic>")
.load()

Streaming ETL
Now that the stream is set up, we can start doing the required ETL on it to extract meaningful insights. Notice that streamingInputDF is a DataFrame. Since DataFrames are essentially an untyped Dataset of rows, we can perform similar operations to them.

val streamingSelectDF = 
streamInputDF
.select(get_json_object(($"value").cast("string"),$"zip").alias("countByZip"))
.groupBy($"countByZip")
.count()

display(streamingSelectDF)

Windowing
Now that we have parse, select, groupBy and count queries continuously executing, what if we want to find out traffic per zip code for a 10 minute window interval, with sliding duration of 5 minutes starting 2 minutes past the hour?

In this case, the incoming JSON contains timestamp in ‘hittime,’ so let’s use that to query the traffic per each 10 minute window.

Note that in Structured Streaming, windowing is considered a groupBy operation. The pie charts below represents each 10 minute window.

> import org.apache.spark.sql.functions._

val streamingSelectDf=
streamInputDF
.select(get_json_object(($"value").cast("string"),$"zip").alias("zip")
         ,get_json_object(($"value").cast("string"),$"hittime").alias("hittime") )
.groupBy($"zip",window($"hittime".cast("timestamp"),"10 minute","5 minute","2 minute"))
.count()

display(streamingSelectDF)
