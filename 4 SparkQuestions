Q. What is difference between reduce and reduceByKey
=====================================================
Ans. 
Reduce :
is for aggregation of built-in datatypes for sum,multi which is commutative and associative 
Can not apply on complex data structure
reduce must pull the entire dataset down into a single location because it is reducing to one final value.

ReduceByKey: works on key and value. 
can apply on complex data structure
is one value for each key. And since this action can be run on each machine locally first then it can remain an RDD and have further transformations done on its dataset.
Merge the values for each key using an associative and commutative reduce function



Q. Converting a Spark Dataframe to a Scala Map collection
=====================================================
https://stackoverflow.com/questions/36895396/converting-a-spark-dataframe-to-a-scala-map-collection/36896210#36896210
https://sparkbyexamples.com/spark/spark-sql-map-functions/
https://sparkbyexamples.com/spark/using-lit-and-typedlit-to-add-a-literal-or-constant-to-spark-dataframe/


Q. How to control driver and executor variable
=====================================================================


Q. What is difference between Coalesce and repartition?
==============================================================
https://medium.com/@mrpowers/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4

coalesce
-----------
The coalesce algorithm moved the data from Partition B to Partition A and moved the data from Partition D to Partition C. The data in Partition A and Partition C does not move with the coalesce algorithm. This algorithm is fast in certain situations because it minimizes data movement.

You can try to increase the number of partitions with coalesce, but it won’t work!
val numbersDf3 = numbersDf.coalesce(6)
numbersDf3.rdd.partitions.size // => 4

repartition
--------------
The repartition method can be used to either increase or decrease the number of partitions in a DataFrame.
Partition ABC contains data from Partition A, Partition B, Partition C, and Partition D. Partition XYZ also contains data from each original partition. The repartition algorithm does a full data shuffle and equally distributes the data among the partitions. It does not attempt to minimize data movement like the coalesce algorithm.

The repartition method can be used to increase the number of partitions as well.
val bartDf = numbersDf.repartition(6)
bartDf.rdd.partitions.size // => 6

Real World Example
--------------------
Suppose you have a data lake that contains 2 billion rows of data (1TB) split in 13,000 partitions.
You’d like to create a data puddle that’s a random sampling of one millionth of the data lake. The data puddle will be used in development and the data lake will be reserved for production grade code. You’d like to write the data puddle out to S3 for easy access.
Here’s how you’d structure the code:

val dataPuddle = dataLake.sample(true, 0.000001)
dataPuddle.write.parquet("s3a://my_bucket/puddle/")

Spark doesn’t adjust the number of partitions when a large DataFrame is filtered, so the dataPuddle will also have 13,000 partitions. The dataPuddle only contains 2,000 rows of data, so a lot of the partitions will be empty. It’s not efficient to read or write thousands of empty text files to S3 — we should improve this code by repartitioning.

val dataPuddle = dataLake.sample(true, 0.000001)
val goodPuddle = dataPuddle.repartition(4)
goodPuddle.write.parquet("s3a://my_bucket/puddle/")

Why did we choose 4 partitions for the data puddle?
------------------------------------------------------------
The data is a million times smaller, so we reduce the number of partitions by a million and keep the same amount of data per partition. 13,000 partitions / 1,000,000 = 1 partition (rounded up). We used 4 partitions so the data puddle can leverage the parallelism of Spark.
In general, you can determine the number of partitions by multiplying the number of CPUs in the cluster by 2, 3, or 4 (see more here and here).
number_of_partitions = number_of_cpus * 4

If you’re writing the data out to a file system, you can choose a partition size that will create reasonable sized files (100MB). Spark will optimize the number of partitions based on the number of clusters when the data is read.

Why did we use the repartition method instead of coalesce?
---------------------------------------------------------------
A full data shuffle is an expensive operation for large data sets, but our data puddle is only 2,000 rows. The repartition method returns equal sized text files, which are more efficient for downstream consumers.
Actual performance improvement
It took 241 seconds to count the rows in the data puddle when the data wasn’t repartitioned (on a 5 node cluster). It only took 2 seconds to count the data puddle when the data was partitioned — that’s a 124x speed improvement!


Q. Apache Storm vs Spark Streaming
===========================================
i. Processing Model
Storm: It supports true stream processing model through core storm layer.
Spark Streaming: Apache Spark Streaming is a wrapper over Spark batch processing.

ii. Primitives
Storm: It provides a very rich set of primitives to perform tuple level process at intervals of a stream (filters, functions). Aggregations over messages in a stream are possible through group by semantics. It supports left join, right join, inner join (default) across the stream.
Spark Streaming: It provides 2 wide varieties of operators. First is Stream transformation operators that transform one DStream into another DStream. Second is output operators that write information to external systems. The previous includes stateless operators (filter, map, mapPartitions, union, distinct than on) still as stateful window operators (countByWindow, reduceByWindow then on).

iii. State Management
Storm: Core Storm by default doesn’t offer any framework level support to store any intermediate bolt output (the result of user operation) as a state. Hence, any application has to create/update its own state as and once required.
Spark Streaming: The underlying Spark by default treats the output of every RDD operation(Transformations and Actions) as an intermediate state. It stores it as RDD. Spark Streaming permits maintaining and changing state via updateStateByKey API. A pluggable method couldn’t be found to implement state within the external system.

iv. Message Delivery Guarantees (Handling message level failures)
Storm: It supports 3 message processing guarantees: at least once, at-most-once and exactly once. Storm’s reliability mechanisms are distributed, scalable, and fault-tolerant.
Spark Streaming: Apache Spark Streaming defines its fault tolerance semantics, the guarantees provided by the recipient and output operators. As per the Apache Spark architecture, the incoming data is read and replicated in different Spark executor’s nodes. This generates failure scenarios data received but may not be reflected. It handles fault tolerance differently in the case of worker failure and driver failure.

v. Fault Tolerance (Handling process/node level failures)
Storm: Storm is intended with fault-tolerance at its core. Storm daemons (Nimbus and Supervisor) are made to be fail-fast (that means that method self-destructs whenever any sudden scenario is encountered) and stateless (all state is unbroken in Zookeeper or on disk).
Spark Streaming: The Driver Node (an equivalent of JT) is SPOF. If driver node fails, then all executors will be lost with their received and replicated in-memory information. Hence, Spark Streaming uses data checkpointing to get over from driver failure.

link for details
https://data-flair.training/blogs/apache-storm-vs-spark-streaming/

3. Conclusion – Apache Storm vs Spark Streaming
Hence, the difference between Apache Storm vs Spark Streaming shows that Apache Storm is a solution for real-time stream processing. But Storm is very complex for developers to develop applications. Very few resources available in the market for it.
Storm can solve only one type of problem i.e Stream processing. But the industry needs a generalized solution which can solve all the types of problems. For example Batch processing, stream processing interactive processing as well as iterative processing. Here Apache Spark comes into limelight which is a general purpose computation engine. It can handle any type of problem. Apart from this Apache Spark is much too easy for developers and can integrate very well with Hadoop.

