Stream Processing Technic:

Tuple at-a-time processing: This processing is also referred to as ‘tuple at-a-time processing’. In tuple at-a-time processing, data flows in the form a stream, where a stream is an unordered sequence of various tuples. Each event generates specific data that is represented as a tuple and is moved across systems as a stream of data. In such type of processing, the events are handled instantly as the tuples arrive.

 

Micro-batch processing: This is very similar to batch processing, except for the fact that the batch is tiny. In micro-batch processing, the batch size generally depends on a few events gathered over a short time span, say a few milliseconds. Micro batches can also be considered a set of tuples grouped to form one micro set.
Generally, we use tuple at-a-time processing when we want the results extremely fast. Micro-batch processing trades off high latency for a low cost.

SparkStorm follow tuple processing
SparkStream follow micro batch processing


Characteristics of Real-Time Stream Processing
==================================================

Read Json from kafka and do the processing as DataFram:
val sqlContext = new SQLContext(sc)
val stream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](
                  ssc, kafkaParams, Set("myTopicName"))

stream.foreachRDD(
  rdd => {
     val dataFrame = sqlContext.read.json(rdd.map(_._2)) //converts json to DF
     //do your operations on this DF. You won't even require a model class.
        })
-----------
> use Play Framework's library for Json.
-------------
import play.api.libs.json._
import org.apache.spark.streaming.kafka.KafkaUtils

case class MyClass(field1: String,
                   field2: Int)

implicit val myClassFormat = Json.format[MyClass]

val kafkaParams = Map[String, String](...here are your params...)    
KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](
  ssc, kafkaParams, Set("myTopicName"))
  .map(m => Json.parse(m._2).as[MyClass])
