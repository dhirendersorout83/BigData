Programming in HBase => Bulk Loading Data into HBase

package bulkload;

/*
 * This MR Program Bulk loads data from AirLine.csv file present on HDFS to HBase
 * 
 * Takes three input- 
 * 		args[0] input file path (hdfs path)
 * 		args[1] ouput file path (hdfs path, where the hfile will intially be stored)
 * 		args[2] Name of the table to be created.
 * */


import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.KeyValue;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Admin;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.RegionLocator;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;
import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class Driver {

	public static void main(String[] args) throws Exception {
		
		
		if(args.length!=3) {
			System.out.println("Please provide proper inputs");
			return;
		}
		
		// the input file is stored in HDFS
		Path inputPath = new Path(args[0]);
		// outout dir is also in HDFS
		Path outputPath = new Path(args[1]);
		// name of the table we want to create
		String tableName = args[2];
		
		Configuration conf = HBaseConfiguration.create();
		System.out.println("Connecting to the server...");
		Connection con = ConnectionFactory.createConnection(conf);
		Admin admin = con.getAdmin();
		System.out.println("Connected");
		
		// check if table alredy exists
		if(admin.tableExists(TableName.valueOf(tableName))){
			System.out.println("'"+tableName+"' table name already exists!");
			return;						
		}

		/******************** Creating Table ******************/
		System.out.println("Creating table named " + tableName);
		HTableDescriptor htable = new HTableDescriptor(TableName.valueOf(tableName));
		htable.addFamily(new HColumnDescriptor("time"));
		htable.addFamily(new HColumnDescriptor("flight"));
		htable.addFamily(new HColumnDescriptor("status"));
		admin.createTable(htable);
		System.out.println("table " + tableName + " is Created");
		/******************************************************/

		conf.set("hbase.table.name", tableName);
		
		// Create a job object
		Job job = new Job(conf, "Airline Data Bulk Load");
		
		// set mapper class
		job.setJarByClass(MyMapper.class);
		job.setMapperClass(MyMapper.class);
		job.setMapOutputKeyClass(ImmutableBytesWritable.class);
		job.setMapOutputValueClass(KeyValue.class);
		job.setInputFormatClass(TextInputFormat.class);
		job.getConfiguration().setInt("mapreduce.input.lineinputformat.linespermap", 100);
		
		// Describe table to Map-Reduce job
		Table table = con.getTable(TableName.valueOf(tableName));
		RegionLocator regionLocator = con.getRegionLocator(TableName.valueOf(tableName));
		HFileOutputFormat2.configureIncrementalLoad(job, table, regionLocator);
		
		// set path variable
		FileInputFormat.addInputPath(job, inputPath);
		FileOutputFormat.setOutputPath(job, outputPath);
		
		job.waitForCompletion(true);

		System.out.println("The HFile is created.");

		// Loading data into HBase
		System.out.println("Loading data to HBase...");
		LoadIncrementalHFiles loader = new LoadIncrementalHFiles(conf);
		loader.doBulkLoad(outputPath, admin, table, regionLocator);
		System.out.println("Driver Terminated");

	}
}

Mapper
===========
package bulkload;


import java.io.IOException;
import org.apache.hadoop.hbase.KeyValue;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MyMapper extends Mapper<LongWritable, Text, ImmutableBytesWritable, KeyValue> {

	ImmutableBytesWritable hkey = new ImmutableBytesWritable();
	KeyValue kv;

	@Override
	protected void setup(Context context) throws IOException, InterruptedException {}

	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

		String[] vals = null;

		try {
			
			//csv file
			vals = value.toString().split(",");
			
			//ignore header
			if(vals[0].equals("ID")) {
				return;
			}
			
			//row key
			hkey.set(Bytes.toBytes(vals[0]));
			/**********************************************************************************************************/
			kv = new KeyValue(hkey.get(), Bytes.toBytes("time"), Bytes.toBytes("year"), Bytes.toBytes(vals[1]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("time"), Bytes.toBytes("month"), Bytes.toBytes(vals[2]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("time"), Bytes.toBytes("dayofMonth"), Bytes.toBytes(vals[3]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("time"), Bytes.toBytes("dayofWeek"), Bytes.toBytes(vals[4]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("time"), Bytes.toBytes("depTime"), Bytes.toBytes(vals[5]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("time"), Bytes.toBytes("schDepTime"), Bytes.toBytes(vals[6]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("time"), Bytes.toBytes("arrTime"), Bytes.toBytes(vals[7]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("time"), Bytes.toBytes("schArrTime"), Bytes.toBytes(vals[8]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("time"), Bytes.toBytes("arrDelay"), Bytes.toBytes(vals[15]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("time"), Bytes.toBytes("depDelay"), Bytes.toBytes(vals[16]));
			context.write(hkey, kv);
			/**********************************************************************************************************/
			
			/**********************************************************************************************************/
			kv = new KeyValue(hkey.get(), Bytes.toBytes("flight"), Bytes.toBytes("uniqCarrier"), Bytes.toBytes(vals[9]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("flight"), Bytes.toBytes("flightNum"), Bytes.toBytes(vals[10]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("flight"), Bytes.toBytes("tailNum"), Bytes.toBytes(vals[11]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("flight"), Bytes.toBytes("origin"), Bytes.toBytes(vals[17]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("flight"), Bytes.toBytes("dest"), Bytes.toBytes(vals[18]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("flight"), Bytes.toBytes("distance"), Bytes.toBytes(vals[19]));
			context.write(hkey, kv);
			/**********************************************************************************************************/
			
			/**********************************************************************************************************/
			kv = new KeyValue(hkey.get(), Bytes.toBytes("status"), Bytes.toBytes("cancelled"), Bytes.toBytes(vals[22]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("status"), Bytes.toBytes("canCode"), Bytes.toBytes(vals[23]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("status"), Bytes.toBytes("diverted"), Bytes.toBytes(vals[24]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("status"), Bytes.toBytes("carrDelay"), Bytes.toBytes(vals[25]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("status"), Bytes.toBytes("weatherDelay"), Bytes.toBytes(vals[26]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("status"), Bytes.toBytes("nasDelay"), Bytes.toBytes(vals[27]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("status"), Bytes.toBytes("securityDelay"), Bytes.toBytes(vals[28]));
			context.write(hkey, kv);
			kv = new KeyValue(hkey.get(), Bytes.toBytes("status"), Bytes.toBytes("lateAircraftDelay"), Bytes.toBytes(vals[29]));
			context.write(hkey, kv);
			/**********************************************************************************************************/
			
				
		} catch (Exception e) {
			System.out.println("\nException in map function\n");
			System.out.println(e.getMessage());
		}


	}
}
