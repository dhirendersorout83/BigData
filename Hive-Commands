1. To Enable dynamic partition and buckting, use below commands 
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions=3000;
set hive.exec.max.dynamic.partitions.pernode=3000;
set hive.enforce.bucketing=true ;
==============================================
You can configure Hive LLAP using "hive.llap.execution.mode" which is by default set to none. In this case, Hive will not use LLAP. Other configurations are as follows:
hive.llap.execution.mode=all; In this case, Hive will run all the queries using LLAP if possible and if LLAP is unable to run a particular query, it will be processed by Tez
hive.llap.execution.mode=only; In this case, Hive will run all the queries using LLAP but if LLAP is unable to run a query, it will not fall back to Tez and will fail
hive.llap.execution.mode=auto; In this case, Hive will decide whether to use LLAP or Tez for a particular query
hive.llap.execution.mode=none; In this case, Hive won't use LLAP at all. Hence, all queries are processed using Tez containers.

To find out the current execution engine of your Hive setup you can run the below command:
set hive.execution.engine;
If your current execution engine is something else then Tez, you can use the below command to set the execution engine as Tez(Tez is case insensitive).
set hive.execution.engine=tez;
To use LLAP as the execution engine you can use the below command:

set hive.execution.engine=llap;
=======================================
vectorized configuration.
set hive.vectorized.execution.enabled=true/false;
=============================================
The log files are stored in a different sub-directory in the logs directory in HDFS each time the Spark program runs. To consider data stored in the sub-directories while creating HIVE tables, you need to set the following properties to TRUE.
set hive.mapred.supports.subdirectories=TRUE;
set mapred.input.dir.recursive=TRUE;
===============================================================
2. Dynamic Partitioning in Hive

Dynamic Partitioning in Hive

You need to download the “customer.dat” given below the video.
Note: You should download the file to your local system if you are using Quickstart VM. If you
are using Hue, you can upload the file to HDFS as shown in the video in previous session.

a. To create table use the below query
-----------------------------------------------
CREATE TABLE customer(customer_fname VARCHAR(64),customer_lname
VARCHAR(64),customer_addr STRING,city VARCHAR(64),state
VARCHAR(64),country VARCHAR(64)) ROW FORMAT DELIMITED FIELDS
TERMINATED BY ‘,’ STORED AS TEXTFILE;

create external table if not exists airline_data ( year string ,
month int , dayofmonth int , dayofweek int , depart bigint , scheduledepart bigint , 
arrival bigint , schedulearrival bigint , carrierid string , flightno int , tailno string ,
actualelapsed bigint , crselapsed bigint , airtime bigint , arrdealy int , depdealy int,
origin string , destination string , distance int , taxitime int , cancel tinyint , 
cancelcode string, diverted tinyint , carrierdelay int , weatherdelay int , nasdelay int,
securitydealy int , lateaircraft int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
   "separatorChar" = "\t",
   "quoteChar"     = "'",
   "escapeChar"    = "\\"
)  
location 'hdfs://34.236.145.193:8080/user/admin/data_2004-08.csv'

or

CREATE EXTERNAL TABLE IF NOT EXISTS OrderDelivery(
OrderID VARCHAR(6),
CustomerID VARCHAR(7),
Details  MAP<VARCHAR(5), INT>,
CustomerName  STRING,
HouseNo INT,
Street STRING,
City STRING,
State STRING,
PostalCode VARCHAR(8))
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '#'
MAP KEYS TERMINATED BY '|'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
tblproperties ("skip.header.line.count"="1");

b. To load data into the table use the below command
-----------------------------------------------------
LOAD DATA INPATH 'yourdatapath' INTO TABLE customer ;

Note: If you are using the VM and data is on your local machine use the below command

LOAD DATA LOCAL INPATH 'yourdatapath' INTO TABLE customer ;

Also, note that you will need to provide path to the data file of the particular country and state
which you mention in the last line. Similarly you can load data for other countries also.

c. By default, dynamic partitioning is disabled in Hive to prevent accidental partition
-----------------------------------------------------
creation. To enable, we use:

set hive.exec.dynamic.partition =true;
set hive.exec.dynamic.partition.mode=nonstrict;

d. Create a partitioned table using the below query
-----------------------------------------------------
CREATE TABLE partitioned_customer(customer_fname
varchar(64),customer_lname varchar(64),customer_addr string,city
varchar(64))PARTITIONED BY (country VARCHAR(64),state VARCHAR(64));

e. Load data into the partitioned table using the previously created normal table
-----------------------------------------------------
Insert into table partitioned_customer partition(country,state)
select
customer_fname,customer_lname,customer_addr,city,country,state from
customer;

f. To show all the partitions
-----------------------------------------------------
show partitions partitioned_customer;

g. Simple select query using partition
-----------------------------------------------------
Select * from partitioned_customer where state='az' and country='usa'
;

h. If you want to drop any partitions, it can be done as:
-----------------------------------------------------
ALTER Table partitioned_customer DROP IF EXISTS
PARTITION(country='usa', state='nw',);

i. If you need to change the file to be referred by a partition you can use the below query
-----------------------------------------------------
ALTER TABLE customer_partitioned PARTITION (country='usa',state='az')
SET LOCATION 'newpath';
ALTER TABLE OrderDelivery SET LOCATION ‘/test/PizzaOrders/’;

========================================================
3. Lateral view example for using complex in select statement with other columns
SELECT p.id,p.productname,colors.colorselection FROM default.products P
LATERAL VIEW EXPLODE(p.productcoloroptions) colors as colorselection;
===================================================
4. hive built-in
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF


=========================================================
5. write hive query output to directory
Explanation of the query:
Insert overwrite is written so that if the file is already loaded then it will update the file in the second time run.
You have to define a local directory which was created in the first step. The output file will be available under the created directory.
Look at the line where fields terminated is written, there we have mentioned colon (: ) as we want columns to be separated by a colon delimiter. You can change it to comma (‘,’) or pipe (‘|’) as per your requirement.
We are taking all the data of infostore table so ‘*’ is mentioned in the select query. You can also add filter condition if you wish to transfer certain records.
Please refer below screenshot for reference.

mkdir /root/local_bdp/posts/export-hive-data-into-file/output
INSERT OVERWRITE LOCAL DIRECTORY '/root/local_bdp/posts/export-hive-data-into-file/output'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
SELECT * FROM bdp.infostore;

======================================================================
6. Create Hive table to read parquet files from parquet/avro schema

CREATE TABLE avro_test ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS AVRO TBLPROPERTIES ('avro.schema.url'='myHost/myAvroSchema.avsc'); 
CREATE EXTERNAL TABLE parquet_test LIKE avro_test STORED AS PARQUET LOCATION 'hdfs://myParquetFilesPath';




======================================================================
Try at home
1. insert NA to int type cloumn. clean up hive data before inserting.
   NULL will be populated in table column if there is type mismatch.
2. try to create parquet file from avro file
   create external table xx(no int, name string)
     row format serde 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
     STORED AS INPUTFORMAT
    'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
    OUTPUTFORMAT
    org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
    TBLPROPERTIES ('avro.schema.url'='hdfs/path/to/avro_schema_file.avsc',
    "skip.header.line.count"="1"); 
   or
   CREATE TABLE avro_test ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' 
   STORED AS AVRO TBLPROPERTIES ('avro.schema.url'='myHost/myAvroSchema.avsc'); 
   
   CREATE EXTERNAL TABLE parquet_test LIKE avro_test STORED AS PARQUET LOCATION 'hdfs://myParquetFilesPath';
   or
   
3. Add jar /opt/cloudera/parcels/CDH/lib/hive-hcatalog/share/hcatalog/hive-hcatalog-core-1.1.0-cdh5.15.0.jar;
4. scp -i locationofpemfile locationofdownloadeddataset (secure copy command)


