Introducing RDDs
According to the first white paper released for RDDs, an RDD is a distributed memory abstraction that lets programmers perform in-memory computations on large clusters, in a fault-tolerant manner. The acronym 'RDD' can be expanded as Resilient Distributed Dataset. Some important features of an RDD are as follows:

The term 'RDD' is expanded as Resilient Distributed Dataset, where 'Resilient' refers to fault-tolerant, 'Distributed' refers to data that resides across multiple interconnected nodes, and 'Dataset' is a collection of partitioned data.

RDDs are Spark Core’s abstraction for working with data. In other words, an RDD is a core object that is often used to deal with data in Spark.

An RDD is a fundamental data structure of Spark that is immutable and read-only.

RDDs can include any Python, Java, or Scala objects, or even user-defined classes.

Each dataset is divided into logical partitions that may be computed on different nodes of a cluster. Then, under the hood, Spark distributes the data contained in the RDD internally, among the clusters, and parallelises the operation that you perform on it.


RDDs are broadly classified into two categories: 
==================================================
Basic RDDs:  All the data items in the RDD is a single value.
Paired RDDs:  All the data items in the RDD is a key-value pair.

A basic RDD can be created in two ways in Java. One of the ways of creating an RDD is by converting a Java list to RDD using the parallelize() method defined in SparkContext class.

 

List<Integer> inputIntegers = Arrays.asList(10, 20, 30, 40, 50);
JavaRDD<Integer> integerRdd = sc.parallelize(inputIntegers);

 

This approach is handy but not very practical in real life scenario because the entire data in the "inputIntegers" list should fit in the driver’s memory before it is distributed across the cluster. The other way of creating a basic RDD is by reading data from a file stored in an external filesystem such as HDFS. The textFile() method defined in SparkContext class is used to read a file.

 

JavaRDD<String> ratings = sc.textFile("in/u.data");
 

In the above code "in/u.data" is the relative address of the file "u.data". 

 

Paired RDDs are created by transforming a basic RDD to pairedRDD using various transformation operations such as mapToPair and flatMapToPair. You will learn these operations in detail in the upcoming sessions.

b. In Scala language
========================
As similar to the previous example here also we need to return tuples. Furthermore, this will make available the functions of keyed data. Also, to offer the extra key or value functions, an implicit conversion on Spark RDD of tuples exists.

Transformation Operations
-------------------------
groupByKey
The groupbykey operation generally groups all the values with the same key.
rdd.groupByKey()

reduceByKey(fun)
Here, the reduceByKey operation generally combines values with the same key.
add.reduceByKey( (x, y) => x + y)

mapValues(func)
Even without changing the key, mapValues operation applies a function to each value of a paired RDD of spark.
rdd.mapValues(x => x+1)

keys()
Keys() operation generally returns a spark RDD of just the keys.
rdd.keys()

values()
values() operation generally returns an RDD of just the values.
rdd.values()

sortByKey()
Similarly, the sortByKey operation generally returns an RDD sorted by the key.
rdd.sortByKey()

Action Operations
==================
As similar as RDD transformations, there are same RDD actions available on spark pair RDD. However, paired RDDs also attains some additional actions of spark. Basically, those leverages the advantage of data which is of keyvalue nature. Let’s discuss some of the action methods below, like

countByKey()
Through countByKey operation, we can count the number of elements for each key.
rdd.countByKey()

collectAsMap()
Here, collectAsMap() operation helps to collect the result as a map to provide easy lookup.
rdd.collectAsMap()

lookup(key)
Moreover, it returns all values associated with the provided key.
rdd.lookup()

