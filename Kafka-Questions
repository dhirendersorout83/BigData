Q. What is difference between producer, consumer and stream
Kafka Stream component built to support ETL type of message transformation. Means to input stream from topic , transform and output to other topic. It support real-time processing and same time support advance analytic features such as aggregation, windowing , join etc.

Where to use Consumer - Producer:
------------------------------------
If there are single consumer , consume message process but not spill to other topic.
As point 1 if have just producer producing message the we don't need to Kafka Stream.
If consumer message from one Kafka cluster but publish to different Kafka cluster topic. In that case even you can use Kafka Stream but you have to use separate Producer to publish message tp different cluster. Or simply use Kafka Consumer - Producer mechanism.
Batch processing - if there is requirement to collect message or kind of batch processing its good to use normal traditional way.

Where to use Kafka Stream:
--------------------------
If you consume message from one topic , transform and publish to other topic Kafka Stream is best suited.
Realtime processing, realtime analytic and Machine learning.
Stateful transformation such as aggregation, join, window etc.
Planning to use local state store or mounted store such as Portworx etc.
Achieve Exactly one processing semantic and auto defined fault tolerance.

Q. How to change kafka retention period
bin/kafka-topics.sh --zookeeper zk.yoursite.com --alter --topic as-access --config retention.ms=86400000

//You can check whether the configuration is properly applied with the following command.
bin/kafka-topics.sh --describe --zookeeper zk.yoursite.com --topic as-access

//The following is the right way to alter topic config as of Kafka 0.10.2.0 as Topic config alter operations have been deprecated for bin/kafka-topics.sh:
bin/kafka-configs.sh --zookeeper <zk_host> --alter --entity-type topics --entity-name test_topic --add-config retention.ms=86400000

Q. how to flush data form kafka?
Default period is 1 week. and if you want ot flush out data, we can do it in following ways:

1. flush entire messages for topic , we can delete the topic.

bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic topicname
2. flush message from kafka queue, we can delete the data from kafka servers log. refer below link

https://community.cloudera.com/t5/Support-Questions/How-to-clean-up-purge-Kafka-queues/td-p/95721

Q. If we have multiple partitions and while producing we dont pass partition than in which partition message would land?

Q. what is the logic to chose partiton nos?

Q. how to map partitions to comsumers?

Q. hoe to run multiple consumer threads in kafka for topic?
link https://dzone.com/articles/kafka-consumer-and-multi-threading

Q. how to commitCurrent to commit offset
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(100);
    for (ConsumerRecord<String, String> record : records) {
        System.out.printf("topic = %s, partition = %d, offset =
            %d, customer = %s, country = %s\n",
            record.topic(), record.partition(),
            record.offset(), record.key(), record.value()); 1
    }
    try {
        consumer.commitSync(); 2
    } catch (CommitFailedException e) {
        log.error("commit failed", e) 3
    }
}
The drawback is that while commitSync() will retry the commit until it either succeeds or encounters a nonretriable failure, commitAsync() will not retry. The reason it does not retry is that by the time commitAsync() receives a response from the server, there may have been a later commit that was already successful.
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(100);
    for (ConsumerRecord<String, String> record : records) {
        System.out.printf("topic = %s, partition = %s,
        offset = %d, customer = %s, country = %s\n",
        record.topic(), record.partition(), record.offset(),
        record.key(), record.value());
    }
    consumer.commitAsync(new OffsetCommitCallback() {
        public void onComplete(Map<TopicPartition,
        OffsetAndMetadata> offsets, Exception e) {
            if (e != null)
                log.error("Commit failed for offsets {}", offsets, e);
        }
    }); 1
}
